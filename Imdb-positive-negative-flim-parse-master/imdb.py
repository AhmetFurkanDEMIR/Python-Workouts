# -*- coding: utf-8 -*-
"""imdb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CfxeuDV_rKq9aYhq03FrlWN0XZqiBxjL
"""

from keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) # num_word parametresi ile ensık tekrar eden 10000 örneği saklayıp nadir örnekleri göz ardı ettik.

print(train_data[0]) #flim kriterlerini bulunduran liste değişkeni

print(train_labels[0]) # 0 = olumlu kriter, 1 = olumsuz kriter.

max(max(i) for i in train_data) # en çok kullanılan 10000 kritiği kullanacağımız için 10000 i geçmeyecektir.

word_index = imdb.get_word_index() # word_index kelimeleri bir tam sayı indekse eşleyen sözlüktür.

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()]) # Tam sayı indeksleri kelimelere dönüştürür.

decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]]) 
# index değerini 3 atlamamızın sebebi 0,1,2 sırayla yapılan sıfır eklemelerini dizinin başlangıcı ve bilinmeyenleri kodlamak için ayrılmıştır.
print(decoded_review)

# içerisinde sayı bulunan listeleri sinir ağlarına gönderemezsiniz. Listelerinizi tensörlere dönüştürmelisiniz.

# Tam sayı dizilerini ikili matris olarak kodlamak.

import numpy as np

def vectorize_sequences(sequences, dimension=10000):

  # tüm elemanları 0 olan matris.
  results = np.zeros((len(sequences), dimension))

  for i, sequence in enumerate(sequences):
    
    results[i, sequence] = 1. # results[i] ' nin istenen indekslerini 1 yapar

  return results

x_train = vectorize_sequences(train_data) # eğitim vektör verisi

x_test = vectorize_sequences(test_data) # test vektör verisi

#sonuçların nasıl görüldüğüne bakalım
print(x_train[0])

# etiketleri vektör haline getirmek

y_train = np.asarray(train_labels).astype("float32")

y_test = np.asarray(test_labels).astype("float32")

from keras import models
from keras import layers
# model tanımlamanın iki yolu vardır, biz 1. yolu seçtik
# 1 =) Sequential sınıfı (en sık kullanılan birbirini takip eden katmanlar şeklinde tasarımlar için.)
# 2 =) functional API (yönlü ve döngüsüz katmanlar kullanarak istediğiniz şekilde modeller için)
model = models.Sequential()
#üç katmanlı ağ
model.add(layers.Dense(16, activation="relu"))
model.add(layers.Dense(16, activation="relu"))
model.add(layers.Dense(1, activation="sigmoid"))

from keras import optimizers

model.compile(optimizer="rmsprop", # en iyileme algoritması
              loss="binary_crossentropy", # kayıp fonksiyonu
              metrics=["acc"]) # doğru sınıflandırmanın toplam sınıf sayısına oranı

# veri seti oluşturma

x_val = x_train[:10000]
partial_x_train = x_train[10000:]

y_val = y_train[:10000]
partial_y_train = y_train[10000:]

history = model.fit(partial_x_train, partial_y_train, epochs=5, batch_size=512, validation_data=(x_val,y_val)) #validation_data = doğrulama veri seti

history_dict = history.history # eğitim boyunca neler olup bittiğini saklar.
history_dict.keys() # [eğitim_doğruluk, eğitim_kayıp, model_kayıp, model_doğruluk]

import matplotlib.pyplot as plt

# Eğitim ve doğrulama kaybı

loss_values = history_dict["loss"]

val_loss_values = history_dict["val_loss"]

epochs = range(1, len(loss_values) + 1)

plt.plot(epochs, loss_values, "bo", label="Egitim_kaybi") # "bo" mavi nokta için
plt.plot(epochs, val_loss_values, "b", label="Dogruluk_kaybi") # "b" mavi düz çizgi için.
plt.title("Egitim ve dogruluk kaybi")
plt.xlabel("Epoklar")
plt.ylabel("Kayip")
plt.legend()

plt.show()

# eğitim ve doğrulama başarımı

plt.clf() # şekli temizleme

acc = history_dict["acc"]

val_acc = history_dict["val_acc"]

plt.plot(epochs, acc, "bo", label="Egitim_basarimi")
plt.plot(epochs, val_acc, "b", label="Dogruluk_basarimi")

plt.title("Egitim ve dogruluk basarimi")
plt.xlabel("Epoklar")
plt.ylabel("Basarim")
plt.legend()

plt.show()

# sonuclar

results = model.evaluate(x_test, y_test) # 82 basari elde ettik.
print(results)

model.predict(x_test) # modeli eğittikten sonraonu bu metodla pratik olarak kullanabilirsiniz.

